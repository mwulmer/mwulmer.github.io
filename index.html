<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Maximilian Ulmer</title>

  <meta name="author" content="Maximilian Ulmer">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/r2d2.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Maximilian Ulmer</name>
                  </p>
                  <p>Hello! I am a Research Scientist at the <a
                      href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-8017/">Institute of Robotics and
                      Mechatronics</a> of the <a href="https://www.dlr.de/de">German Aerospace Center (DLR)</a> and a
                    PhD candidate at <a href="https://www.kit.edu/">Karlsruhe Institute of Technology</a> advised by
                    Prof. Rudolph Triebel. My research is focused on camera-based 6D pose estimation for robotic
                    manipulation.
                  </p>
                  <p>
                    Prior, I worked with Prof. Sami Haddadin at the <a href="https://www.ei.tum.de/en/welcome/">Munich
                      Institute of Robotics and Machine Intelligence</a> of <a href="https://www.tum.de/en/">TUM</a>
                    with
                    a focus on robotic manipulation and reinforcement learning.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:max.ulmer@tum.de">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=FgFPar8AAAAJ&hl=en&oi=ao">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/maxwulmer">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/mwulmer">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/MaxUlmer.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/MaxUlmer_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <h1>Publications</h1>
              <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/iros2023.png" alt="blind-date" width="160" height="135">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2303.13241">
                    <papertitle>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics</papertitle>
                  </a>
                  <br>
                  <strong>Maximilian Ulmer</strong>,
                  Maximilian Durner,
                  Martin Sundermeyer,
                  Manuel Stoiber,
                  and Rudolph Triebel
                  <br>
                  <em>IROS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2303.13241">paper</a> /
                  <a href="https://github.com/DLR-RM/eagernet-speedplus">code</a> /
                  <a href="https://youtube.com">video</a>
                  <p></p>
                  <p>Perform highly accurate 6D object pose estimation from only grayscale and an approximate 3D model
                    of
                    the target in orbit. <strong>Best overall result in the <a
                        href="https://kelvins.esa.int/pose-estimation-2021/">SPEED+</a> satellite pose estimation
                      challenge.</strong></p>
                </td>
              </tr>
            </tbody>
            <tbody>
              <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/aforce2022.png" width="160px" height="100px">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2110.09904">
                    <papertitle>Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space
                    </papertitle>
                  </a>
                  <br>
                  <strong>Maximilian Ulmer</strong>,
                  Elie Aljalbout,
                  Sascha Schwarz,
                  and Sami Haddadin
                  <br>
                  <em>arXiv preprint 2022</em>
                  <br>
                  <a href="https://arxiv.org/abs/2110.09904">paper</a> /
                  <a href="https://youtube.com">video</a>
                  <p></p>
                  <p> Introduces a novel adaptive force-impedance action space which allows RL agents to learn complex
                    manipulations skills faster while improving energy consumption and safety.
                  </p>
                </td>
              </tr>
            </tbody>
            <tbody>
              <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/icra2021.png" alt="blind-date" width="160" height="80">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9811663">
                    <papertitle>Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning
                    </papertitle>
                  </a>
                  <br>
                  Elie Aljalbout,
                  <strong>Maximilian Ulmer</strong>
                  and Rudolph Triebel
                  <br>
                  <em>ICRA</em>, 2022
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9811663">paper</a>
                  <p></p>
                  <p> Improve reinformcent learning agents exploration capability with state representations learning.
                  </p>
                </td>
              </tr>
            </tbody>
            <tbody>
              <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cure_icra2022.png" alt="blind-date" width="140" height="140">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2109.13588">
                    <papertitle>Making Curiosity Explicit in Vision-based RL
                    </papertitle>
                  </a>
                  <br>
                  <strong>Maximilian Ulmer*</strong>,
                  Elie Aljalbout*,
                  and Rudolph Triebel
                  <br>
                  <em>ICRA</em> 2021, Workshop on Curios Robots
                  <br>
                  <a href="https://arxiv.org/abs/2109.13588">paper</a>
                  <p></p>
                  <p> Incentivise agents to explore state space they haven't seen through visual feedback.
                  </p>
                </td>
              </tr>
            </tbody>
            <tbody>
              <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
                <!--<td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hotdog.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hotdog.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>-->
                <td style="padding:20px;width:25%;vertical-align:middle">

                  <img src="images/obstacle_avoidance.png" alt="blind-date" width="160" height="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/obstacle-avoidance/home">
                    <papertitle>Learning Vision-based Reactive Policies for Obstacle Avoidance</papertitle>
                  </a>
                  <br>
                  <a href="https://eliealjalbout.github.io/">Elie Aljalbout</a>,
                  Ji Chen,
                  <a href="https://www.msrm.tum.de/rsi/team/wissenschaftliche-mitarbeiter/ritt-konstantin/">Konstantin
                    Ritt</a>,
                  <strong>Maximilian Ulmer</strong>,
                  <a href="https://www.professoren.tum.de/haddadin-sami">Sami Haddadin</a>
                  <br>
                  <em>CoRL</em>, 2020
                  <br>
                  <a href="https://sites.google.com/view/obstacle-avoidance/home">project page</a> /
                  <a href="https://youtu.be/FGa7xNpSchM">video</a> /
                  <a href="https://arxiv.org/abs/2010.16298">arXiv</a>
                  <p></p>
                  <p>Learn obstacle avoidance policies while maintaining closed-loop responsiveness required for
                    critical applications like human-robot interaction.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h1>Teaching</h1>
                  <h2>Technical University of Munich</h2>
                  <p><strong><a href="https://www.msrm.tum.de/rsi/lehre/advanced-robot-control-and-learning/"> EI71056:
                        Advanced Robot Control and Learning</a></strong> [Winter 2019] [Winter 2020]</p>
                  <p><strong>Student Robothon: "Coastal Cleanup: Robotics-aided Climate Protection"</strong> [Winter
                    2019]
                    <br>
                    Head Organizer with <a
                      href="https://www.msrm.tum.de/en/rsi/team/research-assistants/kern-elena/   ">Elena Kern</a> / <a
                      href="https://youtu.be/uuRjdOSwBDk">video</a> by <a href="https://twitter.com/kuoyi_chao">Kuo-Yi
                      Chao</a>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <p style="text-align: right;">Credits to <a href="https://jonbarron.info/">Jon Barron</a> </p>
        </td>
      </tr>

  </table>
</body>

</html>